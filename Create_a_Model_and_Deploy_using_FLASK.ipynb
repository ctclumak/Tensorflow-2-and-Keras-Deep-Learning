{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create a Model and Deploy using FLASK.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNH2oyqJlqosk0V2r5fqiWT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctclumak/Tensorflow-2-and-Keras-Deep-Learning/blob/master/Create_a_Model_and_Deploy_using_FLASK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuqT0CYM6hii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d51c5c72-a1e4-4da4-e2f0-82288525fa9a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEHblIao71_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Iris = pd.read_csv(\"iris.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg-f5IG28Dmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f34123b8-7aa5-4987-c6a6-861c33b0b291"
      },
      "source": [
        "Iris.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width species\n",
              "0           5.1          3.5           1.4          0.2  setosa\n",
              "1           4.9          3.0           1.4          0.2  setosa\n",
              "2           4.7          3.2           1.3          0.2  setosa\n",
              "3           4.6          3.1           1.5          0.2  setosa\n",
              "4           5.0          3.6           1.4          0.2  setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1U0uwtc8GuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate features vs target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4FNJ7un8V9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = Iris.drop(\"species\", axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23sd8giQ8aBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = Iris[\"species\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibikBMa48d2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b897b32-5d3b-4854-98e6-2c51e2c1cbd9"
      },
      "source": [
        "y.unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZlvIh08fOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding the features\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "y = encoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxyss5v18wR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train test split and scaler the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D-joltr84Nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb82a1d7-2307-41b2-e5cc-dccedcfa368c"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOOH6xd99Kka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_X_train = scaler.transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRLeOxsd9aOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units = 4, activation=\"relu\", input_shape=[4,]))\n",
        "model.add(Dense(units = 3, activation=\"softmax\")) # we use \"softmax\" because it is a multi class classification\n",
        "\n",
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWlJ7eBy-G0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec8f30f6-ae0c-45d2-d20b-b292a6993eb3"
      },
      "source": [
        "# train the model\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(patience = 10)\n",
        "\n",
        "model.fit(x = scaled_X_train, y = y_train, epochs=200,\n",
        "          validation_data = (scaled_X_test, y_test), callbacks = [early_stop])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 120 samples, validate on 30 samples\n",
            "Epoch 1/200\n",
            "120/120 [==============================] - 1s 5ms/sample - loss: 1.0875 - accuracy: 0.3500 - val_loss: 1.1791 - val_accuracy: 0.2667\n",
            "Epoch 2/200\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 1.0804 - accuracy: 0.3500 - val_loss: 1.1712 - val_accuracy: 0.2667\n",
            "Epoch 3/200\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 1.0743 - accuracy: 0.3500 - val_loss: 1.1637 - val_accuracy: 0.2667\n",
            "Epoch 4/200\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 1.0685 - accuracy: 0.3500 - val_loss: 1.1565 - val_accuracy: 0.2667\n",
            "Epoch 5/200\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 1.0624 - accuracy: 0.3500 - val_loss: 1.1499 - val_accuracy: 0.2667\n",
            "Epoch 6/200\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 1.0571 - accuracy: 0.3500 - val_loss: 1.1430 - val_accuracy: 0.2667\n",
            "Epoch 7/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 1.0515 - accuracy: 0.3500 - val_loss: 1.1365 - val_accuracy: 0.2667\n",
            "Epoch 8/200\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 1.0462 - accuracy: 0.3500 - val_loss: 1.1302 - val_accuracy: 0.2667\n",
            "Epoch 9/200\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 1.0409 - accuracy: 0.3500 - val_loss: 1.1240 - val_accuracy: 0.2667\n",
            "Epoch 10/200\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 1.0352 - accuracy: 0.3500 - val_loss: 1.1184 - val_accuracy: 0.2667\n",
            "Epoch 11/200\n",
            "120/120 [==============================] - 0s 176us/sample - loss: 1.0299 - accuracy: 0.3500 - val_loss: 1.1125 - val_accuracy: 0.2667\n",
            "Epoch 12/200\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 1.0248 - accuracy: 0.3500 - val_loss: 1.1067 - val_accuracy: 0.2667\n",
            "Epoch 13/200\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 1.0196 - accuracy: 0.3500 - val_loss: 1.1009 - val_accuracy: 0.2667\n",
            "Epoch 14/200\n",
            "120/120 [==============================] - 0s 211us/sample - loss: 1.0146 - accuracy: 0.3500 - val_loss: 1.0951 - val_accuracy: 0.2667\n",
            "Epoch 15/200\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 1.0096 - accuracy: 0.3500 - val_loss: 1.0897 - val_accuracy: 0.2667\n",
            "Epoch 16/200\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 1.0051 - accuracy: 0.3500 - val_loss: 1.0840 - val_accuracy: 0.2667\n",
            "Epoch 17/200\n",
            "120/120 [==============================] - 0s 173us/sample - loss: 1.0005 - accuracy: 0.3500 - val_loss: 1.0787 - val_accuracy: 0.2667\n",
            "Epoch 18/200\n",
            "120/120 [==============================] - 0s 296us/sample - loss: 0.9957 - accuracy: 0.3500 - val_loss: 1.0740 - val_accuracy: 0.2667\n",
            "Epoch 19/200\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.9915 - accuracy: 0.3500 - val_loss: 1.0693 - val_accuracy: 0.2667\n",
            "Epoch 20/200\n",
            "120/120 [==============================] - 0s 171us/sample - loss: 0.9875 - accuracy: 0.3500 - val_loss: 1.0646 - val_accuracy: 0.2667\n",
            "Epoch 21/200\n",
            "120/120 [==============================] - 0s 218us/sample - loss: 0.9832 - accuracy: 0.3500 - val_loss: 1.0605 - val_accuracy: 0.2667\n",
            "Epoch 22/200\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.9795 - accuracy: 0.3500 - val_loss: 1.0563 - val_accuracy: 0.2667\n",
            "Epoch 23/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.9759 - accuracy: 0.3500 - val_loss: 1.0522 - val_accuracy: 0.2667\n",
            "Epoch 24/200\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.9724 - accuracy: 0.3500 - val_loss: 1.0484 - val_accuracy: 0.2667\n",
            "Epoch 25/200\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.9692 - accuracy: 0.3500 - val_loss: 1.0445 - val_accuracy: 0.2667\n",
            "Epoch 26/200\n",
            "120/120 [==============================] - 0s 175us/sample - loss: 0.9660 - accuracy: 0.3500 - val_loss: 1.0413 - val_accuracy: 0.2667\n",
            "Epoch 27/200\n",
            "120/120 [==============================] - 0s 238us/sample - loss: 0.9629 - accuracy: 0.3500 - val_loss: 1.0380 - val_accuracy: 0.2667\n",
            "Epoch 28/200\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.9602 - accuracy: 0.3500 - val_loss: 1.0348 - val_accuracy: 0.2667\n",
            "Epoch 29/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.9573 - accuracy: 0.3500 - val_loss: 1.0317 - val_accuracy: 0.2667\n",
            "Epoch 30/200\n",
            "120/120 [==============================] - 0s 183us/sample - loss: 0.9547 - accuracy: 0.3500 - val_loss: 1.0285 - val_accuracy: 0.2667\n",
            "Epoch 31/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.9521 - accuracy: 0.3500 - val_loss: 1.0255 - val_accuracy: 0.2667\n",
            "Epoch 32/200\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.9495 - accuracy: 0.3917 - val_loss: 1.0226 - val_accuracy: 0.4000\n",
            "Epoch 33/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.9469 - accuracy: 0.4917 - val_loss: 1.0200 - val_accuracy: 0.4000\n",
            "Epoch 34/200\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.9445 - accuracy: 0.5500 - val_loss: 1.0174 - val_accuracy: 0.4000\n",
            "Epoch 35/200\n",
            "120/120 [==============================] - 0s 214us/sample - loss: 0.9421 - accuracy: 0.5583 - val_loss: 1.0148 - val_accuracy: 0.4000\n",
            "Epoch 36/200\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.9398 - accuracy: 0.5917 - val_loss: 1.0126 - val_accuracy: 0.5000\n",
            "Epoch 37/200\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.9375 - accuracy: 0.5917 - val_loss: 1.0103 - val_accuracy: 0.5000\n",
            "Epoch 38/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.9353 - accuracy: 0.6167 - val_loss: 1.0078 - val_accuracy: 0.5000\n",
            "Epoch 39/200\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.9332 - accuracy: 0.6333 - val_loss: 1.0056 - val_accuracy: 0.5333\n",
            "Epoch 40/200\n",
            "120/120 [==============================] - 0s 180us/sample - loss: 0.9310 - accuracy: 0.6333 - val_loss: 1.0033 - val_accuracy: 0.5333\n",
            "Epoch 41/200\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.9289 - accuracy: 0.6417 - val_loss: 1.0008 - val_accuracy: 0.5333\n",
            "Epoch 42/200\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.9268 - accuracy: 0.6417 - val_loss: 0.9985 - val_accuracy: 0.5333\n",
            "Epoch 43/200\n",
            "120/120 [==============================] - 0s 197us/sample - loss: 0.9247 - accuracy: 0.6500 - val_loss: 0.9961 - val_accuracy: 0.5333\n",
            "Epoch 44/200\n",
            "120/120 [==============================] - 0s 187us/sample - loss: 0.9226 - accuracy: 0.6500 - val_loss: 0.9939 - val_accuracy: 0.5333\n",
            "Epoch 45/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.9206 - accuracy: 0.6500 - val_loss: 0.9918 - val_accuracy: 0.5333\n",
            "Epoch 46/200\n",
            "120/120 [==============================] - 0s 195us/sample - loss: 0.9185 - accuracy: 0.6500 - val_loss: 0.9896 - val_accuracy: 0.5333\n",
            "Epoch 47/200\n",
            "120/120 [==============================] - 0s 174us/sample - loss: 0.9165 - accuracy: 0.6667 - val_loss: 0.9875 - val_accuracy: 0.5667\n",
            "Epoch 48/200\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.9144 - accuracy: 0.6667 - val_loss: 0.9854 - val_accuracy: 0.6000\n",
            "Epoch 49/200\n",
            "120/120 [==============================] - 0s 183us/sample - loss: 0.9123 - accuracy: 0.6667 - val_loss: 0.9835 - val_accuracy: 0.6000\n",
            "Epoch 50/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.9103 - accuracy: 0.6667 - val_loss: 0.9816 - val_accuracy: 0.6000\n",
            "Epoch 51/200\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.9081 - accuracy: 0.6750 - val_loss: 0.9795 - val_accuracy: 0.6000\n",
            "Epoch 52/200\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.9060 - accuracy: 0.6750 - val_loss: 0.9775 - val_accuracy: 0.6000\n",
            "Epoch 53/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.9040 - accuracy: 0.6750 - val_loss: 0.9755 - val_accuracy: 0.6000\n",
            "Epoch 54/200\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.9019 - accuracy: 0.6833 - val_loss: 0.9733 - val_accuracy: 0.6000\n",
            "Epoch 55/200\n",
            "120/120 [==============================] - 0s 173us/sample - loss: 0.8997 - accuracy: 0.6833 - val_loss: 0.9713 - val_accuracy: 0.6000\n",
            "Epoch 56/200\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.8977 - accuracy: 0.6833 - val_loss: 0.9693 - val_accuracy: 0.6000\n",
            "Epoch 57/200\n",
            "120/120 [==============================] - 0s 164us/sample - loss: 0.8956 - accuracy: 0.6833 - val_loss: 0.9672 - val_accuracy: 0.6000\n",
            "Epoch 58/200\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.8935 - accuracy: 0.6833 - val_loss: 0.9653 - val_accuracy: 0.6000\n",
            "Epoch 59/200\n",
            "120/120 [==============================] - 0s 200us/sample - loss: 0.8914 - accuracy: 0.6833 - val_loss: 0.9630 - val_accuracy: 0.6000\n",
            "Epoch 60/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.8893 - accuracy: 0.6833 - val_loss: 0.9610 - val_accuracy: 0.6000\n",
            "Epoch 61/200\n",
            "120/120 [==============================] - 0s 170us/sample - loss: 0.8872 - accuracy: 0.6833 - val_loss: 0.9590 - val_accuracy: 0.6000\n",
            "Epoch 62/200\n",
            "120/120 [==============================] - 0s 287us/sample - loss: 0.8851 - accuracy: 0.6833 - val_loss: 0.9567 - val_accuracy: 0.6000\n",
            "Epoch 63/200\n",
            "120/120 [==============================] - 0s 198us/sample - loss: 0.8829 - accuracy: 0.6833 - val_loss: 0.9546 - val_accuracy: 0.6000\n",
            "Epoch 64/200\n",
            "120/120 [==============================] - 0s 194us/sample - loss: 0.8808 - accuracy: 0.6833 - val_loss: 0.9525 - val_accuracy: 0.6000\n",
            "Epoch 65/200\n",
            "120/120 [==============================] - 0s 227us/sample - loss: 0.8787 - accuracy: 0.6833 - val_loss: 0.9504 - val_accuracy: 0.6000\n",
            "Epoch 66/200\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.8766 - accuracy: 0.6833 - val_loss: 0.9484 - val_accuracy: 0.6000\n",
            "Epoch 67/200\n",
            "120/120 [==============================] - 0s 179us/sample - loss: 0.8745 - accuracy: 0.6833 - val_loss: 0.9462 - val_accuracy: 0.6000\n",
            "Epoch 68/200\n",
            "120/120 [==============================] - 0s 178us/sample - loss: 0.8724 - accuracy: 0.6833 - val_loss: 0.9442 - val_accuracy: 0.6000\n",
            "Epoch 69/200\n",
            "120/120 [==============================] - 0s 208us/sample - loss: 0.8702 - accuracy: 0.6833 - val_loss: 0.9421 - val_accuracy: 0.6000\n",
            "Epoch 70/200\n",
            "120/120 [==============================] - 0s 213us/sample - loss: 0.8680 - accuracy: 0.6833 - val_loss: 0.9399 - val_accuracy: 0.6000\n",
            "Epoch 71/200\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.8659 - accuracy: 0.6833 - val_loss: 0.9377 - val_accuracy: 0.6000\n",
            "Epoch 72/200\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.8638 - accuracy: 0.6833 - val_loss: 0.9353 - val_accuracy: 0.6000\n",
            "Epoch 73/200\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.8616 - accuracy: 0.6833 - val_loss: 0.9330 - val_accuracy: 0.6000\n",
            "Epoch 74/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.8596 - accuracy: 0.6833 - val_loss: 0.9307 - val_accuracy: 0.6000\n",
            "Epoch 75/200\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.8574 - accuracy: 0.6833 - val_loss: 0.9285 - val_accuracy: 0.6000\n",
            "Epoch 76/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.8553 - accuracy: 0.6833 - val_loss: 0.9263 - val_accuracy: 0.6000\n",
            "Epoch 77/200\n",
            "120/120 [==============================] - 0s 188us/sample - loss: 0.8531 - accuracy: 0.6833 - val_loss: 0.9242 - val_accuracy: 0.6000\n",
            "Epoch 78/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.8510 - accuracy: 0.6833 - val_loss: 0.9220 - val_accuracy: 0.6000\n",
            "Epoch 79/200\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 0.8490 - accuracy: 0.6833 - val_loss: 0.9201 - val_accuracy: 0.6000\n",
            "Epoch 80/200\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.8467 - accuracy: 0.6833 - val_loss: 0.9178 - val_accuracy: 0.6000\n",
            "Epoch 81/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.8446 - accuracy: 0.6833 - val_loss: 0.9156 - val_accuracy: 0.6000\n",
            "Epoch 82/200\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.8425 - accuracy: 0.6833 - val_loss: 0.9135 - val_accuracy: 0.6000\n",
            "Epoch 83/200\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.8404 - accuracy: 0.6833 - val_loss: 0.9114 - val_accuracy: 0.6000\n",
            "Epoch 84/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.8382 - accuracy: 0.6833 - val_loss: 0.9092 - val_accuracy: 0.6000\n",
            "Epoch 85/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.8361 - accuracy: 0.6833 - val_loss: 0.9070 - val_accuracy: 0.6000\n",
            "Epoch 86/200\n",
            "120/120 [==============================] - 0s 165us/sample - loss: 0.8340 - accuracy: 0.6833 - val_loss: 0.9048 - val_accuracy: 0.6000\n",
            "Epoch 87/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.8319 - accuracy: 0.6833 - val_loss: 0.9026 - val_accuracy: 0.6000\n",
            "Epoch 88/200\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.8299 - accuracy: 0.6833 - val_loss: 0.9003 - val_accuracy: 0.6000\n",
            "Epoch 89/200\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.8277 - accuracy: 0.6833 - val_loss: 0.8981 - val_accuracy: 0.6000\n",
            "Epoch 90/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.8255 - accuracy: 0.6833 - val_loss: 0.8959 - val_accuracy: 0.6000\n",
            "Epoch 91/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.8236 - accuracy: 0.6833 - val_loss: 0.8937 - val_accuracy: 0.6000\n",
            "Epoch 92/200\n",
            "120/120 [==============================] - 0s 165us/sample - loss: 0.8214 - accuracy: 0.6833 - val_loss: 0.8917 - val_accuracy: 0.6000\n",
            "Epoch 93/200\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.8192 - accuracy: 0.6917 - val_loss: 0.8895 - val_accuracy: 0.6000\n",
            "Epoch 94/200\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.8172 - accuracy: 0.6917 - val_loss: 0.8874 - val_accuracy: 0.6000\n",
            "Epoch 95/200\n",
            "120/120 [==============================] - 0s 170us/sample - loss: 0.8150 - accuracy: 0.6917 - val_loss: 0.8853 - val_accuracy: 0.6000\n",
            "Epoch 96/200\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.8129 - accuracy: 0.6917 - val_loss: 0.8833 - val_accuracy: 0.6000\n",
            "Epoch 97/200\n",
            "120/120 [==============================] - 0s 164us/sample - loss: 0.8109 - accuracy: 0.6917 - val_loss: 0.8811 - val_accuracy: 0.6000\n",
            "Epoch 98/200\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.8087 - accuracy: 0.6917 - val_loss: 0.8790 - val_accuracy: 0.6000\n",
            "Epoch 99/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.8068 - accuracy: 0.6917 - val_loss: 0.8769 - val_accuracy: 0.6000\n",
            "Epoch 100/200\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.8046 - accuracy: 0.6917 - val_loss: 0.8747 - val_accuracy: 0.6333\n",
            "Epoch 101/200\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.8025 - accuracy: 0.6917 - val_loss: 0.8726 - val_accuracy: 0.6333\n",
            "Epoch 102/200\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.8004 - accuracy: 0.6917 - val_loss: 0.8705 - val_accuracy: 0.6333\n",
            "Epoch 103/200\n",
            "120/120 [==============================] - 0s 164us/sample - loss: 0.7984 - accuracy: 0.6917 - val_loss: 0.8684 - val_accuracy: 0.6333\n",
            "Epoch 104/200\n",
            "120/120 [==============================] - 0s 165us/sample - loss: 0.7963 - accuracy: 0.7000 - val_loss: 0.8664 - val_accuracy: 0.6333\n",
            "Epoch 105/200\n",
            "120/120 [==============================] - 0s 173us/sample - loss: 0.7942 - accuracy: 0.7000 - val_loss: 0.8644 - val_accuracy: 0.6333\n",
            "Epoch 106/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.7922 - accuracy: 0.7000 - val_loss: 0.8623 - val_accuracy: 0.6333\n",
            "Epoch 107/200\n",
            "120/120 [==============================] - 0s 178us/sample - loss: 0.7901 - accuracy: 0.7000 - val_loss: 0.8602 - val_accuracy: 0.6333\n",
            "Epoch 108/200\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.7881 - accuracy: 0.7000 - val_loss: 0.8581 - val_accuracy: 0.6333\n",
            "Epoch 109/200\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.7861 - accuracy: 0.7000 - val_loss: 0.8558 - val_accuracy: 0.6333\n",
            "Epoch 110/200\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.7839 - accuracy: 0.7000 - val_loss: 0.8537 - val_accuracy: 0.6333\n",
            "Epoch 111/200\n",
            "120/120 [==============================] - 0s 242us/sample - loss: 0.7818 - accuracy: 0.7000 - val_loss: 0.8515 - val_accuracy: 0.6333\n",
            "Epoch 112/200\n",
            "120/120 [==============================] - 0s 180us/sample - loss: 0.7798 - accuracy: 0.7000 - val_loss: 0.8493 - val_accuracy: 0.6333\n",
            "Epoch 113/200\n",
            "120/120 [==============================] - 0s 209us/sample - loss: 0.7777 - accuracy: 0.7000 - val_loss: 0.8472 - val_accuracy: 0.6333\n",
            "Epoch 114/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.7757 - accuracy: 0.7000 - val_loss: 0.8451 - val_accuracy: 0.6333\n",
            "Epoch 115/200\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.7736 - accuracy: 0.7000 - val_loss: 0.8429 - val_accuracy: 0.6333\n",
            "Epoch 116/200\n",
            "120/120 [==============================] - 0s 196us/sample - loss: 0.7716 - accuracy: 0.7000 - val_loss: 0.8408 - val_accuracy: 0.6333\n",
            "Epoch 117/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.7698 - accuracy: 0.7083 - val_loss: 0.8387 - val_accuracy: 0.6333\n",
            "Epoch 118/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.7676 - accuracy: 0.7083 - val_loss: 0.8364 - val_accuracy: 0.6333\n",
            "Epoch 119/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.7655 - accuracy: 0.7083 - val_loss: 0.8342 - val_accuracy: 0.6333\n",
            "Epoch 120/200\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.7638 - accuracy: 0.7083 - val_loss: 0.8320 - val_accuracy: 0.6333\n",
            "Epoch 121/200\n",
            "120/120 [==============================] - 0s 198us/sample - loss: 0.7615 - accuracy: 0.7083 - val_loss: 0.8300 - val_accuracy: 0.6333\n",
            "Epoch 122/200\n",
            "120/120 [==============================] - 0s 165us/sample - loss: 0.7595 - accuracy: 0.7083 - val_loss: 0.8280 - val_accuracy: 0.6333\n",
            "Epoch 123/200\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 0.7576 - accuracy: 0.7083 - val_loss: 0.8260 - val_accuracy: 0.6333\n",
            "Epoch 124/200\n",
            "120/120 [==============================] - 0s 236us/sample - loss: 0.7555 - accuracy: 0.7083 - val_loss: 0.8239 - val_accuracy: 0.6333\n",
            "Epoch 125/200\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.7535 - accuracy: 0.7083 - val_loss: 0.8217 - val_accuracy: 0.6333\n",
            "Epoch 126/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.7515 - accuracy: 0.7083 - val_loss: 0.8194 - val_accuracy: 0.6333\n",
            "Epoch 127/200\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.7494 - accuracy: 0.7083 - val_loss: 0.8173 - val_accuracy: 0.6333\n",
            "Epoch 128/200\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.7474 - accuracy: 0.7083 - val_loss: 0.8152 - val_accuracy: 0.6333\n",
            "Epoch 129/200\n",
            "120/120 [==============================] - 0s 192us/sample - loss: 0.7454 - accuracy: 0.7083 - val_loss: 0.8131 - val_accuracy: 0.6333\n",
            "Epoch 130/200\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.7434 - accuracy: 0.7083 - val_loss: 0.8110 - val_accuracy: 0.6333\n",
            "Epoch 131/200\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.7414 - accuracy: 0.7083 - val_loss: 0.8087 - val_accuracy: 0.6333\n",
            "Epoch 132/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.7395 - accuracy: 0.7083 - val_loss: 0.8065 - val_accuracy: 0.6333\n",
            "Epoch 133/200\n",
            "120/120 [==============================] - 0s 236us/sample - loss: 0.7375 - accuracy: 0.7083 - val_loss: 0.8045 - val_accuracy: 0.6333\n",
            "Epoch 134/200\n",
            "120/120 [==============================] - 0s 211us/sample - loss: 0.7355 - accuracy: 0.7083 - val_loss: 0.8024 - val_accuracy: 0.6333\n",
            "Epoch 135/200\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.7335 - accuracy: 0.7167 - val_loss: 0.8003 - val_accuracy: 0.6333\n",
            "Epoch 136/200\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.7315 - accuracy: 0.7167 - val_loss: 0.7981 - val_accuracy: 0.6333\n",
            "Epoch 137/200\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 0.7297 - accuracy: 0.7167 - val_loss: 0.7960 - val_accuracy: 0.6333\n",
            "Epoch 138/200\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.7275 - accuracy: 0.7250 - val_loss: 0.7938 - val_accuracy: 0.6333\n",
            "Epoch 139/200\n",
            "120/120 [==============================] - 0s 191us/sample - loss: 0.7256 - accuracy: 0.7250 - val_loss: 0.7916 - val_accuracy: 0.6333\n",
            "Epoch 140/200\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.7237 - accuracy: 0.7250 - val_loss: 0.7894 - val_accuracy: 0.6333\n",
            "Epoch 141/200\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.7217 - accuracy: 0.7250 - val_loss: 0.7874 - val_accuracy: 0.6333\n",
            "Epoch 142/200\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.7197 - accuracy: 0.7250 - val_loss: 0.7857 - val_accuracy: 0.6333\n",
            "Epoch 143/200\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.7177 - accuracy: 0.7250 - val_loss: 0.7837 - val_accuracy: 0.6333\n",
            "Epoch 144/200\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.7160 - accuracy: 0.7250 - val_loss: 0.7819 - val_accuracy: 0.6333\n",
            "Epoch 145/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.7139 - accuracy: 0.7250 - val_loss: 0.7797 - val_accuracy: 0.6333\n",
            "Epoch 146/200\n",
            "120/120 [==============================] - 0s 178us/sample - loss: 0.7119 - accuracy: 0.7250 - val_loss: 0.7775 - val_accuracy: 0.6333\n",
            "Epoch 147/200\n",
            "120/120 [==============================] - 0s 177us/sample - loss: 0.7100 - accuracy: 0.7250 - val_loss: 0.7752 - val_accuracy: 0.6333\n",
            "Epoch 148/200\n",
            "120/120 [==============================] - 0s 197us/sample - loss: 0.7081 - accuracy: 0.7250 - val_loss: 0.7729 - val_accuracy: 0.6333\n",
            "Epoch 149/200\n",
            "120/120 [==============================] - 0s 182us/sample - loss: 0.7060 - accuracy: 0.7250 - val_loss: 0.7709 - val_accuracy: 0.6667\n",
            "Epoch 150/200\n",
            "120/120 [==============================] - 0s 174us/sample - loss: 0.7042 - accuracy: 0.7250 - val_loss: 0.7688 - val_accuracy: 0.6667\n",
            "Epoch 151/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.7021 - accuracy: 0.7250 - val_loss: 0.7667 - val_accuracy: 0.6667\n",
            "Epoch 152/200\n",
            "120/120 [==============================] - 0s 203us/sample - loss: 0.7004 - accuracy: 0.7250 - val_loss: 0.7646 - val_accuracy: 0.7000\n",
            "Epoch 153/200\n",
            "120/120 [==============================] - 0s 184us/sample - loss: 0.6985 - accuracy: 0.7250 - val_loss: 0.7624 - val_accuracy: 0.7000\n",
            "Epoch 154/200\n",
            "120/120 [==============================] - 0s 171us/sample - loss: 0.6964 - accuracy: 0.7250 - val_loss: 0.7602 - val_accuracy: 0.7000\n",
            "Epoch 155/200\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.6945 - accuracy: 0.7250 - val_loss: 0.7581 - val_accuracy: 0.7000\n",
            "Epoch 156/200\n",
            "120/120 [==============================] - 0s 167us/sample - loss: 0.6926 - accuracy: 0.7333 - val_loss: 0.7561 - val_accuracy: 0.7000\n",
            "Epoch 157/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.6907 - accuracy: 0.7333 - val_loss: 0.7541 - val_accuracy: 0.7000\n",
            "Epoch 158/200\n",
            "120/120 [==============================] - 0s 191us/sample - loss: 0.6889 - accuracy: 0.7333 - val_loss: 0.7521 - val_accuracy: 0.7000\n",
            "Epoch 159/200\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.6869 - accuracy: 0.7417 - val_loss: 0.7499 - val_accuracy: 0.7000\n",
            "Epoch 160/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.6850 - accuracy: 0.7417 - val_loss: 0.7479 - val_accuracy: 0.7000\n",
            "Epoch 161/200\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 0.6832 - accuracy: 0.7417 - val_loss: 0.7460 - val_accuracy: 0.7000\n",
            "Epoch 162/200\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.6812 - accuracy: 0.7417 - val_loss: 0.7439 - val_accuracy: 0.7000\n",
            "Epoch 163/200\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.6793 - accuracy: 0.7417 - val_loss: 0.7419 - val_accuracy: 0.7000\n",
            "Epoch 164/200\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.6774 - accuracy: 0.7417 - val_loss: 0.7399 - val_accuracy: 0.7000\n",
            "Epoch 165/200\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.6756 - accuracy: 0.7417 - val_loss: 0.7380 - val_accuracy: 0.7000\n",
            "Epoch 166/200\n",
            "120/120 [==============================] - 0s 171us/sample - loss: 0.6739 - accuracy: 0.7417 - val_loss: 0.7362 - val_accuracy: 0.7000\n",
            "Epoch 167/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.6718 - accuracy: 0.7417 - val_loss: 0.7340 - val_accuracy: 0.7000\n",
            "Epoch 168/200\n",
            "120/120 [==============================] - 0s 193us/sample - loss: 0.6700 - accuracy: 0.7583 - val_loss: 0.7320 - val_accuracy: 0.7333\n",
            "Epoch 169/200\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.6682 - accuracy: 0.7583 - val_loss: 0.7299 - val_accuracy: 0.7333\n",
            "Epoch 170/200\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.6662 - accuracy: 0.7583 - val_loss: 0.7277 - val_accuracy: 0.7333\n",
            "Epoch 171/200\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.6645 - accuracy: 0.7583 - val_loss: 0.7256 - val_accuracy: 0.7333\n",
            "Epoch 172/200\n",
            "120/120 [==============================] - 0s 174us/sample - loss: 0.6626 - accuracy: 0.7583 - val_loss: 0.7235 - val_accuracy: 0.7333\n",
            "Epoch 173/200\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.6608 - accuracy: 0.7583 - val_loss: 0.7217 - val_accuracy: 0.7333\n",
            "Epoch 174/200\n",
            "120/120 [==============================] - 0s 169us/sample - loss: 0.6588 - accuracy: 0.7667 - val_loss: 0.7197 - val_accuracy: 0.7667\n",
            "Epoch 175/200\n",
            "120/120 [==============================] - 0s 192us/sample - loss: 0.6570 - accuracy: 0.7750 - val_loss: 0.7178 - val_accuracy: 0.7667\n",
            "Epoch 176/200\n",
            "120/120 [==============================] - 0s 194us/sample - loss: 0.6552 - accuracy: 0.7833 - val_loss: 0.7159 - val_accuracy: 0.7667\n",
            "Epoch 177/200\n",
            "120/120 [==============================] - 0s 185us/sample - loss: 0.6533 - accuracy: 0.7833 - val_loss: 0.7139 - val_accuracy: 0.8000\n",
            "Epoch 178/200\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.6515 - accuracy: 0.7833 - val_loss: 0.7119 - val_accuracy: 0.8000\n",
            "Epoch 179/200\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.6496 - accuracy: 0.7833 - val_loss: 0.7097 - val_accuracy: 0.8000\n",
            "Epoch 180/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.6480 - accuracy: 0.7917 - val_loss: 0.7077 - val_accuracy: 0.8000\n",
            "Epoch 181/200\n",
            "120/120 [==============================] - 0s 189us/sample - loss: 0.6460 - accuracy: 0.7917 - val_loss: 0.7057 - val_accuracy: 0.8000\n",
            "Epoch 182/200\n",
            "120/120 [==============================] - 0s 173us/sample - loss: 0.6442 - accuracy: 0.7917 - val_loss: 0.7036 - val_accuracy: 0.8000\n",
            "Epoch 183/200\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.6424 - accuracy: 0.7917 - val_loss: 0.7016 - val_accuracy: 0.8000\n",
            "Epoch 184/200\n",
            "120/120 [==============================] - 0s 174us/sample - loss: 0.6407 - accuracy: 0.7917 - val_loss: 0.6996 - val_accuracy: 0.8000\n",
            "Epoch 185/200\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.6388 - accuracy: 0.7917 - val_loss: 0.6978 - val_accuracy: 0.8000\n",
            "Epoch 186/200\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.6371 - accuracy: 0.7917 - val_loss: 0.6961 - val_accuracy: 0.8000\n",
            "Epoch 187/200\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.6353 - accuracy: 0.7917 - val_loss: 0.6944 - val_accuracy: 0.8000\n",
            "Epoch 188/200\n",
            "120/120 [==============================] - 0s 171us/sample - loss: 0.6335 - accuracy: 0.7917 - val_loss: 0.6923 - val_accuracy: 0.8000\n",
            "Epoch 189/200\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.6317 - accuracy: 0.8083 - val_loss: 0.6904 - val_accuracy: 0.8000\n",
            "Epoch 190/200\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.6299 - accuracy: 0.8083 - val_loss: 0.6884 - val_accuracy: 0.8000\n",
            "Epoch 191/200\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.6281 - accuracy: 0.8083 - val_loss: 0.6863 - val_accuracy: 0.8000\n",
            "Epoch 192/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.6265 - accuracy: 0.8083 - val_loss: 0.6842 - val_accuracy: 0.8000\n",
            "Epoch 193/200\n",
            "120/120 [==============================] - 0s 169us/sample - loss: 0.6246 - accuracy: 0.8167 - val_loss: 0.6824 - val_accuracy: 0.8000\n",
            "Epoch 194/200\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.6228 - accuracy: 0.8250 - val_loss: 0.6806 - val_accuracy: 0.8000\n",
            "Epoch 195/200\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.6211 - accuracy: 0.8250 - val_loss: 0.6789 - val_accuracy: 0.8000\n",
            "Epoch 196/200\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.6194 - accuracy: 0.8250 - val_loss: 0.6770 - val_accuracy: 0.8000\n",
            "Epoch 197/200\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.6176 - accuracy: 0.8250 - val_loss: 0.6748 - val_accuracy: 0.8000\n",
            "Epoch 198/200\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.6159 - accuracy: 0.8250 - val_loss: 0.6727 - val_accuracy: 0.8333\n",
            "Epoch 199/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.6142 - accuracy: 0.8250 - val_loss: 0.6707 - val_accuracy: 0.8333\n",
            "Epoch 200/200\n",
            "120/120 [==============================] - 0s 173us/sample - loss: 0.6124 - accuracy: 0.8250 - val_loss: 0.6688 - val_accuracy: 0.8333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1140884fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czJ5lGoC_GQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "2828e456-8e63-44f9-ff1f-b22038ec41e7"
      },
      "source": [
        "#check the performance\n",
        "\n",
        "metrics = pd.DataFrame(model.history.history)\n",
        "metrics[['loss', 'val_loss']].plot()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f113d2790b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD6CAYAAACxrrxPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl67ww4-C7ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bd204da2-4c05-4f72-ff25-d442cbd76347"
      },
      "source": [
        "#check the accuracy metrics\n",
        "metrics[['accuracy', 'val_accuracy']].plot()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f113eac7cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmUKjDF8DJ2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75852988-48b5-4a46-c28b-5332bb44f60a"
      },
      "source": [
        "model.evaluate(scaled_X_test,y_test, verbose = 0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6688003540039062, 0.8333333]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqpdIs2zEO5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if we think the accuracy is good enough, it is 93% here we can just fit_transform all the X data\n",
        " epochs = len(metrics)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWKl_4-sEvFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7a721cc-7e36-4eff-fb87-6ea48f9d6f41"
      },
      "source": [
        "epochs"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPhW8b2XEv1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_X = scaler.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlyewXrBFNON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a new model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units = 4, activation=\"relu\", input_shape=[4,]))\n",
        "model.add(Dense(units = 3, activation=\"softmax\")) # we use \"softmax\" because it is a multi class classification\n",
        "\n",
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SZwy8UBFfWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the scaled_X to the model\n",
        "\n",
        "model.fit(scaled_X,y,epochs = epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ehCOlKqFnZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model\n",
        "model.save(\"final_iris_models.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q294JqGaF6uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the scaler\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwy_jxdnF_OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7adc7801-3346-4678-e554-21ff91bdc63d"
      },
      "source": [
        "joblib.dump(scaler,'iris_scaler.pkl')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iris_scaler.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEwc8NaQGGUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict on a new file\n",
        "\n",
        "## load the model and scaler first\n",
        "from tensorflow.keras.models import load_model\n",
        "flower_model = load_model(\"final_iris_models.h5\")\n",
        "flower_scaler = joblib.load(\"iris_scaler.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heJ9coWrGgKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "f1aa5299-192b-4680-bf29-3d91629f402c"
      },
      "source": [
        "Iris.head(1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width species\n",
              "0           5.1          3.5           1.4          0.2  setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubct0MTSHMmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flower_example = {\"sepal_length\": 5.1,\n",
        "                  \"sepal_width\": 3.5,\n",
        "                  \"petal_length\": 1.4,\n",
        "                  \"petal_width\": 0.2,\n",
        "                    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl9IxbfqIPsM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccff6db8-82f5-4d00-b0f8-4c59176f4fcf"
      },
      "source": [
        "encoder.classes_"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-vMvZVkHfhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_prediction(model,scaler, sample_json):\n",
        "  s_len = sample_json[\"sepal_length\"]\n",
        "  s_wid = sample_json[\"sepal_width\"]\n",
        "  p_len = sample_json[\"petal_length\"]\n",
        "  p_wid = sample_json[\"petal_width\"]\n",
        "\n",
        "  flower = [[s_len,s_wid,p_len, p_wid]]\n",
        "\n",
        "  classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
        "\n",
        "  flower = scaler.transform(flower)\n",
        "\n",
        "  class_ind = model.predict_classes(flower)\n",
        "\n",
        "  return classes[class_ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAYL2k51IqKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea6f9147-918f-4876-9ea6-243955716ed5"
      },
      "source": [
        "return_prediction(flower_model, flower_scaler, flower_example)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa'], dtype='<U10')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acB-C3-2IwDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4gI63ciJErA",
        "colab_type": "text"
      },
      "source": [
        "**CODE FOR DEPLOYMENT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oIhfJS9JHZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "flower_model = load_model(\"final_iris_models.h5\")\n",
        "flower_scaler = joblib.load(\"iris_scaler.pkl\")\n",
        "\n",
        "def return_prediction(model,scaler, sample_json):\n",
        "  s_len = sample_json[\"sepal_length\"]\n",
        "  s_wid = sample_json[\"sepal_width\"]\n",
        "  p_len = sample_json[\"petal_length\"]\n",
        "  p_wid = sample_json[\"petal_width\"]\n",
        "\n",
        "  flower = [[s_len,s_wid,p_len, p_wid]]\n",
        "  classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
        "\n",
        "  flower = scaler.transform(flower)\n",
        "\n",
        "  class_ind = model.predict_classes(flower)\n",
        "\n",
        "  return classes[class_ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSTM0ijgOe1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSK7w0aGOjdb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvOMshzHOfgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP4KycwAOfub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}